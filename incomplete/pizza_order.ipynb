{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chatbot slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = '판교에 지금 주문해줘'\n",
    "output_data = ''\n",
    "\n",
    "request = {\n",
    "    'intent_id':'',\n",
    "    'input_data':input_data,\n",
    "    'request_type':'text',\n",
    "    'story_slot_entity':{},\n",
    "    'output_data':output_data\n",
    "}\n",
    "\n",
    "intent_list = {\n",
    "    '주문':['주문', '배달'],\n",
    "    '예약':['예약', '잡아줘'],\n",
    "    '정보':['정보', '알려']\n",
    "}\n",
    "\n",
    "story_slot_entity = {\n",
    "    '주문':{\n",
    "        '메뉴':None,\n",
    "        '장소':None,\n",
    "        '날짜':None\n",
    "    },\n",
    "    '예약':{\n",
    "        '장소':None,\n",
    "        '날짜':None\n",
    "    },\n",
    "    '정보':{\n",
    "        '대상':None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('판교', 'NNG'), ('에', 'JKB'), ('지금', 'MAG'), ('주문', 'NNG'), ('해', 'XSV+EC'), ('줘', 'VX+EC')]\n"
     ]
    }
   ],
   "source": [
    "preprocessed = mecab.pos(request.get('input_data'))\n",
    "print(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_id = '주문'\n",
    "slot_value = story_slot_entity.get('주문')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_list = ['피자', '햄버거', '치킨']\n",
    "loc_list = ['판교', '야탑', '서현']\n",
    "date_list = ['지금', '내일', '모레']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'메뉴': None, '장소': '판교', '날짜': '지금'}\n"
     ]
    }
   ],
   "source": [
    "for pos_tag in preprocessed:\n",
    "    if pos_tag[1] in ['NNG', 'NNP', 'SL', 'MAG']:\n",
    "        if pos_tag[0] in menu_list:\n",
    "            slot_value['메뉴'] = pos_tag[0]\n",
    "        elif pos_tag[0] in loc_list:\n",
    "            slot_value['장소'] = pos_tag[0]\n",
    "        elif pos_tag[0] in date_list:\n",
    "            slot_value['날짜'] = pos_tag[0]\n",
    "\n",
    "print(story_slot_entity.get('주문'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메뉴 선택해 주세요\n"
     ]
    }
   ],
   "source": [
    "if (None in slot_value.values()):\n",
    "    key_values = \"\"\n",
    "    for key in slot_value.keys():\n",
    "        if (slot_value[key] is None):\n",
    "            key_values = key_values+key+\"\"\n",
    "        output_data = key_values+' 선택해 주세요'\n",
    "else:\n",
    "    output_data = '주문이 완료되었습니다.'\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메뉴 선택해 주세요\n"
     ]
    }
   ],
   "source": [
    "response = {\n",
    "    'intent_id' : '',\n",
    "    'input_data' : input_data,\n",
    "    'request_type' : 'text',\n",
    "    'story_slot_entity' : {},\n",
    "    'output_data' : ''\n",
    "}\n",
    "response['output_data'] = output_data\n",
    "print(response[\"output_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_data = [\n",
    "    ['안녕', '만나서 반가워'],\n",
    "    ['넌 누구니', '나는 AI로봇이란다.'],\n",
    "    ['피자 주문 할게', '음료도 주문해줘'],\n",
    "    ['음료는 뭘로','콜라로 해줘']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(map(lambda x : mecab.morphs(''.join(x)), one_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "train_data = list(itertools.chain.from_iterable(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕', '만나', '서', '반가워', '넌', '누구', '니', '나', '는', 'AI', '로', '봇', '이', '란다', '.', '피자', '주문', '할', '게', '음료', '도', '주문', '해', '줘', '음료', '는', '뭘로', '콜라', '로', '해', '줘']\n"
     ]
    }
   ],
   "source": [
    "print(list(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bucket = np.zeros(len(train_data), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-771e5edc8d4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbucket_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# out이 없음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'out'"
     ]
    }
   ],
   "source": [
    "for word in train_data:\n",
    "    bucket_temp = bucket.copy()\n",
    "    np.out(bucket_temp, train_data.index(word), 1) # out이 없음\n",
    "    print(bucket_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['안녕', '만나', '서', '반가워', '넌', '누구', '니', '나', '는', 'AI', '로', '봇', '이', '란다', '.', '피자', '주문', '할', '게', '음료', '도', '주문', '해', '줘', '음료', '는', '뭘로', '콜라', '로', '해', '줘']]\n"
     ]
    }
   ],
   "source": [
    "train_data = [train_data]\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model check : Word2Vec(vocab=25, size=50, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(train_data,size=50, window=2, min_count=1)\n",
    "#model.build_vocab(train_data)\n",
    "#model.train(train_data)\n",
    "print('model check : {0}'.format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent와 NER 모델을 만들기 위한 Data의 구성 방법  \n",
    "피자주문 / 숙소예약 / 여행정보의 각각의 Entity 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_order = ['판교에 오늘 피자 주문해줘']\n",
    "train_data_reserve = ['오늘 날짜에 호텔 예약 해줄래']\n",
    "train_data_info = ['모레 날짜의 판교 여행 정보 알려줘']\n",
    "\n",
    "get_data_list = train_data_info[0]\n",
    "\n",
    "dict_entity = {\n",
    "    'date' : ['오늘', '내일', '모레'],\n",
    "    'loc' : ['판교', '야탑'],\n",
    "    'menu' : ['피자', '햄버거'],\n",
    "    'hotel' : ['호텔', '여관', '민박'],\n",
    "    'travel' : ['여행', '관광', '카페']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation length is 108\n"
     ]
    }
   ],
   "source": [
    "length = 1\n",
    "for key in list(dict_entity.keys()):\n",
    "    length = length * len(dict_entity[key])\n",
    "print(\"Augmentation length is {0}\".format(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('모레', 'MAG'), ('날짜', 'NNG'), ('의', 'JKG'), ('판교', 'NNG'), ('여행', 'NNG'), ('정보', 'NNG'), ('알려줘', 'VV+EC+VX+EC')]\n"
     ]
    }
   ],
   "source": [
    "morphed_text = mecab.pos(get_data_list)\n",
    "print(morphed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모레 날짜 판교 여행 정보 \n"
     ]
    }
   ],
   "source": [
    "tagged_text = ''\n",
    "for pos_tag in morphed_text:\n",
    "    if(pos_tag[1] in ['NNG', 'MAG', 'NNP', 'SL'] and len(pos_tag[0]) > 1):\n",
    "        feature_value = pos_tag[0]\n",
    "        tagged_text = tagged_text + pos_tag[0] + ' '\n",
    "print(tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagdate 날짜 tagloc tagtravel 정보  \n"
     ]
    }
   ],
   "source": [
    "pattern = ''\n",
    "for word in tagged_text.split(' '):\n",
    "    entity = list(filter(lambda key : word in dict_entity[key], list(dict_entity.keys())))\n",
    "    if (len(entity) > 0):\n",
    "        pattern = pattern + 'tag' + entity[0] + ' '\n",
    "    else:\n",
    "        pattern = pattern + word + ' '\n",
    "print(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_pattern(pattern, dict_entity):\n",
    "    # 입력된 패턴을 List로 바꿈\n",
    "    aug_pattern = pattern.split(' ')\n",
    "    # Augment된 Text List\n",
    "    augmented_text_list = []\n",
    "    # copy를 위한 임시 list\n",
    "    temp_aug = []\n",
    "    for i in range(0, len(aug_pattern)):\n",
    "        # Entity에 해당하는 값일 경우 Entity List를 가져옴\n",
    "        if aug_pattern[i].find('tag') > -1 : # tag가 있으면 0, 없으면 1\n",
    "            dict_list = dict_entity[aug_pattern[i].replace('tag',\"\")]\n",
    "            # 각 Entity 별로 값을 append하면서 pattern 구성\n",
    "            for j in range(0, len(dict_list)):\n",
    "                # 최초 Entity값은 그냥 추가만\n",
    "                if i == 0:\n",
    "                    augmented_text_list.append(dict_list[j] + \" \")\n",
    "                elif j == 1:\n",
    "                    augmented_text_list = list(filter(lambda word : len(word.split(' ')) == i+1, augmented_text_list))\n",
    "                    copy_data_order = augmented_text_list * (len(dict_list)-2)\n",
    "                    augmented_text_list = list(map(lambda x : x + dict_list[j] + \" \", augmented_text_list))\n",
    "                    augmented_text_list = augmented_text_list + temp_aug + copy_data_order\n",
    "                else:\n",
    "                    # list의 수를 체크하여 값을 추가\n",
    "                    temp_aug = list(filter(lambda word : len(word.split(' ')) == i+1, augmented_text_list))\n",
    "                    temp_aug = list(map(lambda x : x + dict_list[j] + \" \", temp_aug))\n",
    "                    # 추가된 list를 위해 기존 값 삭제\n",
    "                    if j != 0:\n",
    "                        augmented_text_list = augmented_text_list[0:len(augmented_text_list) - len(temp_aug)]\n",
    "                    augmented_text_list = augmented_text_list + temp_aug\n",
    "        # Entity 추가 대상이 아닐 경우 Pattern만 추가\n",
    "        else:\n",
    "            augmented_text_list = list(map(lambda x : x + aug_pattern[i] + \" \", augmented_text_list))\n",
    "        # N*N으로 증가시키기 위한 list\n",
    "        temp_aug = augmented_text_list\n",
    "    return augmented_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘 날짜 야탑 관광 정보   ',\n",
       " '내일 날짜 야탑 관광 정보   ',\n",
       " '모레 날짜 야탑 관광 정보   ',\n",
       " '오늘 날짜 판교 관광 정보   ',\n",
       " '내일 날짜 판교 관광 정보   ',\n",
       " '모레 날짜 판교 관광 정보   ',\n",
       " '오늘 날짜 야탑 여행 정보   ',\n",
       " '내일 날짜 야탑 여행 정보   ',\n",
       " '모레 날짜 야탑 여행 정보   ',\n",
       " '오늘 날짜 판교 여행 정보   ',\n",
       " '내일 날짜 판교 여행 정보   ',\n",
       " '모레 날짜 판교 여행 정보   ',\n",
       " '오늘 날짜 야탑 카페 정보   ',\n",
       " '내일 날짜 야탑 카페 정보   ',\n",
       " '모레 날짜 야탑 카페 정보   ',\n",
       " '오늘 날짜 판교 카페 정보   ',\n",
       " '내일 날짜 판교 카페 정보   ',\n",
       " '모레 날짜 판교 카페 정보   ']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_text_list = augmentation_pattern(pattern, dict_entity)\n",
    "augmented_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bio Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_bio_pattern(pattern, dict_entity):\n",
    "    # 입력된 패턴을 list로 바꿈\n",
    "    aug_pattern = pattern.split(' ')\n",
    "    # augment된 text list\n",
    "    augmented_text_list = []\n",
    "    # copy를 위한 임시 list\n",
    "    temp_aug = []\n",
    "    for i in range(0, len(aug_pattern)):\n",
    "        # Entity에 해당하는 값일 경우 Entity List를 가져옴\n",
    "        if aug_pattern[i].find('tag') > -1:\n",
    "            dict_list = dict_entity[aug_pattern[i].replace('tag', \"\")]\n",
    "            bio_tag = aug_pattern[i].replace('tag', \"B_\")\n",
    "            # 각 Entity별로 값을 append하면서 pattern 구성\n",
    "            for j in range(0, len(dict_list)):\n",
    "                # 최초 entity 값은 추가만\n",
    "                if i == 0:\n",
    "                    augmented_text_list.append(bio_tag+\" \")\n",
    "                elif j == 1:\n",
    "                    augmented_text_list = list(filter(lambda word : len(word.split(' ')) == i+1, augmented_text_list))\n",
    "                    copy_data_order = augmented_text_list * (len(dict_list)-2)\n",
    "                    augmented_text_list = list(map(lambda x : x + bio_tag + \" \", augmented_text_list))\n",
    "                    augmented_text_list = augmented_text_list + temp_aug + copy_data_order\n",
    "                else:\n",
    "                    # list의 수를 체크하여 값 추가\n",
    "                    temp_aug = list(filter(lambda word : len(word.split(' ')) == i+1, augmented_text_list))\n",
    "                    temp_aug = list(map(lambda x : x + bio_tag + \" \", temp_aug))\n",
    "                    # 추가된 list를 위해 기존 값 삭제\n",
    "                    if j != 0:\n",
    "                        augmented_text_list = augmented_text_list[0:len(augmented_text_list) - len(temp_aug)]\n",
    "                        augmented_text_list = augmented_text_list + temp_aug\n",
    "        # Entity 추가 대상이 아닐 경우 pattern만 추가\n",
    "        else:\n",
    "            augmented_text_list = list(map(lambda x : x + aug_pattern[i] + \" \", augmented_text_list))\n",
    "        # N*N으로 증가시키기 위한 list\n",
    "        temp_aug = augmented_text_list\n",
    "    return augmented_text_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ',\n",
       " 'B_date 날짜 B_loc B_travel 정보   ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_list = augmentation_bio_pattern(pattern, dict_entity)\n",
    "bio_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER을 위한 Full Train Text 확보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['오늘 날짜 야탑 관광 정보   ',\n",
       "  '내일 날짜 야탑 관광 정보   ',\n",
       "  '모레 날짜 야탑 관광 정보   ',\n",
       "  '오늘 날짜 판교 관광 정보   ',\n",
       "  '내일 날짜 판교 관광 정보   ',\n",
       "  '모레 날짜 판교 관광 정보   ',\n",
       "  '오늘 날짜 야탑 여행 정보   ',\n",
       "  '내일 날짜 야탑 여행 정보   ',\n",
       "  '모레 날짜 야탑 여행 정보   ',\n",
       "  '오늘 날짜 판교 여행 정보   ',\n",
       "  '내일 날짜 판교 여행 정보   ',\n",
       "  '모레 날짜 판교 여행 정보   ',\n",
       "  '오늘 날짜 야탑 카페 정보   ',\n",
       "  '내일 날짜 야탑 카페 정보   ',\n",
       "  '모레 날짜 야탑 카페 정보   ',\n",
       "  '오늘 날짜 판교 카페 정보   ',\n",
       "  '내일 날짜 판교 카페 정보   ',\n",
       "  '모레 날짜 판교 카페 정보   '],\n",
       " ['B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ',\n",
       "  'B_date 날짜 B_loc B_travel 정보   ']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_train_text = [augmented_text_list, bio_list]\n",
    "ner_train_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
